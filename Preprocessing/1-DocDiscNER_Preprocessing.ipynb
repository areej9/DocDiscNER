{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code is for Preprocessing the original (Raw) ShARe-13, ShARe-14 and CADEC to the DocDiscNER Format\n",
        "\n"
      ],
      "metadata": {
        "id": "tKZPv-t4YSM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V30ddmc9fGw_",
        "outputId": "3509a85f-1758-4018-c9df-c995d81a44a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) ShaRe-13 Preprocessing"
      ],
      "metadata": {
        "id": "zCKH_xsOFYIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def extract_spans(annotation_file, text_file):\n",
        "    spans = []\n",
        "    extracted_spans = []\n",
        "    with open(annotation_file, 'r') as ann_file:\n",
        "        for line in ann_file:\n",
        "            line_span = []\n",
        "            parts = line.strip().split(\"||\")\n",
        "            if len(parts) > 5:  #  if there are multipe segments\n",
        "                start_end_offsets = parts[3:]\n",
        "                for i in range(0, len(start_end_offsets), 2):\n",
        "                    start = int(start_end_offsets[i])\n",
        "                    end = int(start_end_offsets[i+1])\n",
        "                    line_span.append((start, end))\n",
        "                merged_span = \",\".join([f\"{start}-{end}\" for start, end in line_span])\n",
        "                spans.append(merged_span)\n",
        "            elif len(parts) == 5:  # if there is only one segment\n",
        "                start = int(parts[3])\n",
        "                end = int(parts[4])\n",
        "                line_span.append((start, end))\n",
        "                spans.append(f\"{start}-{end}\")\n",
        "\n",
        "            with open(text_file, 'r') as txt_file:\n",
        "                text = txt_file.read()\n",
        "\n",
        "            disc_span = \"\"\n",
        "            for start, end in line_span:\n",
        "              disc_span = disc_span + \" \" + text[start:end]\n",
        "            disc_span = disc_span.strip().replace('\\n', ' ')\n",
        "            extracted_spans.append(disc_span)\n",
        "    return extracted_spans, spans\n",
        "\n",
        "\n",
        "def process_data(text_folder, annotation_folder, output_file):\n",
        "    with open(output_file, 'w', newline='') as output:\n",
        "        writer = csv.writer(output)\n",
        "        writer.writerow([\"Report_ID\", \"Text\", \"Spans\", \"Offsets\"])\n",
        "\n",
        "        for ann_file in os.listdir(annotation_folder):\n",
        "            report_id = ann_file.split('.')[0]\n",
        "            annotation_path = os.path.join(annotation_folder, ann_file)\n",
        "            text_path = os.path.join(text_folder, report_id + '.txt')\n",
        "\n",
        "            spans, offsets = extract_spans(annotation_path, text_path)\n",
        "            text = open(text_path, 'r').read()\n",
        "\n",
        "            spans_text = '\\n'.join(spans)\n",
        "            offsets_text = '\\n'.join(offsets)\n",
        "\n",
        "\n",
        "\n",
        "            writer.writerow([report_id, text, spans_text, offsets_text])\n",
        "\n",
        "# paths to the (Raw) ShARe-13  file\n",
        "text_folder = '/content/drive/MyDrive/DocDiscNER/Datasets/Share13/Train/text'\n",
        "annotation_folder = '/content/drive/MyDrive/DocDiscNER/Datasets/Share13/Train/ann'\n",
        "output_file = 'Doc-ShaRe-13.csv'\n",
        "\n",
        "# Process the data\n",
        "process_data(text_folder, annotation_folder, output_file)"
      ],
      "metadata": {
        "id": "UHh-ig6RS8-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now take the resulting output and chunk it in DocDiscNER_Chunker.ipynb"
      ],
      "metadata": {
        "id": "lOsZ5I1FzbuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) ShaRe-14 Preprocessing"
      ],
      "metadata": {
        "id": "xPFs7Ab73IMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def extract_spans(annotation_file, text_file):\n",
        "    spans = []\n",
        "    extracted_spans = []\n",
        "    with open(annotation_file, 'r') as ann_file:\n",
        "        for line in ann_file:\n",
        "            line_span = []\n",
        "            parts = line.strip().split(\"|\")\n",
        "            second_split = parts[1]\n",
        "            if second_split.count(\",\") >= 1:  #  if there are multipe segments\n",
        "                offsets_segments = second_split.split(\",\")\n",
        "                for segment in offsets_segments:\n",
        "                    start, end = map(int, segment.split(\"-\"))\n",
        "                    line_span.append((start, end))\n",
        "                merged_span = \",\".join([f\"{start}-{end}\" for start, end in line_span])\n",
        "                spans.append(merged_span)\n",
        "            else:   #  if there is only one segment\n",
        "                start, end = map(int,  second_split.split(\"-\"))\n",
        "                line_span.append((start, end))\n",
        "                spans.append(f\"{start}-{end}\")\n",
        "\n",
        "            with open(text_file, 'r') as txt_file:\n",
        "                text = txt_file.read()\n",
        "\n",
        "            disc_span = \"\"\n",
        "            for start, end in line_span:\n",
        "              disc_span = disc_span + \" \" + text[start:end]\n",
        "            disc_span = disc_span.strip().replace('\\n', ' ')\n",
        "            extracted_spans.append(disc_span)\n",
        "    # I noticed duplicates in ShaRe-14, this code block is to remove duplicates\n",
        "    seen_tuples = set()\n",
        "\n",
        "    # iterate over indices of spans in reverse order to  remove duplicates\n",
        "    for i in range(len(spans) - 1, -1, -1):\n",
        "        detector = spans[i]\n",
        "        if (detector) in seen_tuples:\n",
        "            # If the tuple is already seen, remove it from both lists\n",
        "            del spans[i]\n",
        "            del extracted_spans[i]\n",
        "        else:\n",
        "            # If the tuple is not seen, add it to the set\n",
        "            seen_tuples.add(detector)\n",
        "    # end of deduplication code segment\n",
        "    # in future, if i want to add CUIs I need to remove this deduplicator\n",
        "    # or i need to figure out way to add CUIs in parallel with spans and extracted_spans\n",
        "    return extracted_spans, spans\n",
        "\n",
        "\n",
        "def process_data(text_folder, annotation_folder, output_file):\n",
        "    with open(output_file, 'w', newline='') as output:\n",
        "        writer = csv.writer(output)\n",
        "        writer.writerow([\"Report_ID\", \"Text\", \"Spans\", \"Offsets\"])\n",
        "\n",
        "        for ann_file in os.listdir(annotation_folder):\n",
        "            report_id = ann_file.split('.')[0]\n",
        "            annotation_path = os.path.join(annotation_folder, ann_file)\n",
        "            text_path = os.path.join(text_folder, report_id + '.txt')\n",
        "\n",
        "            spans, offsets = extract_spans(annotation_path, text_path)\n",
        "            text = open(text_path, 'r').read()\n",
        "\n",
        "            spans_text = '\\n'.join(spans)\n",
        "            offsets_text = '\\n'.join(offsets)\n",
        "\n",
        "\n",
        "\n",
        "            writer.writerow([report_id, text, spans_text, offsets_text])\n",
        "\n",
        "# Provide paths to the (Raw) ShARe-14  file\n",
        "text_folder = '/content/drive/MyDrive/DocDiscNER/Datasets/Share14/test/text'\n",
        "annotation_folder = '/content/drive/MyDrive/DocDiscNER/Datasets/Share14/test/ann'\n",
        "output_file = 'Test-Doc-ShaRe-14'\n",
        "\n",
        "# Process the data\n",
        "process_data(text_folder, annotation_folder, output_file)"
      ],
      "metadata": {
        "id": "XUYSvLZ_3MOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now take the resulting output and chunk it in DocDiscNER_Chunker.ipynb"
      ],
      "metadata": {
        "id": "EZU94Nf6tAxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CADEC Preprocessing"
      ],
      "metadata": {
        "id": "fHpSh1teghJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since CADEC is not lengthy, no need for chunking, we are just going to preprocess CADEC to the DocDiscNER format\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "\n",
        "def extract_spans(annotation_file):\n",
        "    spans = []\n",
        "    extracted_spans = []\n",
        "    with open(annotation_file, 'r') as ann_file:\n",
        "        for line in ann_file:\n",
        "\n",
        "          parts = line.split('\\t')\n",
        "          if len(parts) < 3:\n",
        "            continue\n",
        "          # extract span_offsets and span_text from the parts\n",
        "          span_offsets_text = parts[1].strip()\n",
        "          span_offsets_text = re.sub(r\"CONCEPT_LESS\\s?\", \"\", span_offsets_text)\n",
        "          span_offsets_text = re.sub(r\"\\b\\d{8}\\b\\s?\", \"\", span_offsets_text)\n",
        "          span_offsets_text = re.sub(r\"\\+\\s?\", \"\", span_offsets_text)\n",
        "          span_text = parts[2].strip()\n",
        "\n",
        "          extracted_spans.append(span_text)\n",
        "          spans.append(span_offsets_text)\n",
        "\n",
        "    return extracted_spans, spans\n",
        "\n",
        "\n",
        "def process_data(text_folder, annotation_folder, output_file):\n",
        "    with open(output_file, 'w', newline='') as output:\n",
        "        writer = csv.writer(output)\n",
        "        writer.writerow([\"Report_ID\", \"Text\", \"Spans\", \"Offsets\"])\n",
        "\n",
        "        for ann_file in os.listdir(annotation_folder):\n",
        "            report_id = '.'.join(ann_file.split('.')[:-1])\n",
        "            annotation_path = os.path.join(annotation_folder, ann_file)\n",
        "            text_path = os.path.join(text_folder, report_id + '.txt')\n",
        "\n",
        "            spans, offsets = extract_spans(annotation_path)\n",
        "            text = open(text_path, 'r').read()\n",
        "\n",
        "            spans_text = '\\n'.join(spans)\n",
        "            offsets_text = '\\n'.join(offsets)\n",
        "\n",
        "\n",
        "\n",
        "            writer.writerow([report_id, text, spans_text, offsets_text])\n",
        "\n",
        "#  paths to the (Raw) CADEC  file and meddra annotations\n",
        "text_folder = '/content/drive/MyDrive/DocDiscNER/Datasets/CADEC/text'\n",
        "annotation_folder = '/content/drive/MyDrive/DocDiscNER/Datasets/CADEC/meddra'\n",
        "output_file = 'Doc-CADEC.csv'\n",
        "\n",
        "process_data(text_folder, annotation_folder, output_file)"
      ],
      "metadata": {
        "id": "qVOoX_s0gkrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting CADEC to Train,Val,Test"
      ],
      "metadata": {
        "id": "CF5vKHPxmqBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the splits are based on the IDs provided by (Dai et al.,2020): https://github.com/daixiangau/acl2020-transition-discontinuous-ner/tree/master/data/cadec/split\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Doc-CADEC.csv')\n",
        "with open('/content/Val_ID.txt', 'r') as f:\n",
        "    validation_ids = f.read().splitlines()\n",
        "\n",
        "with open('/content/Test_ID.txt', 'r') as f:\n",
        "    test_ids = f.read().splitlines()\n",
        "\n",
        "validation_set = set(validation_ids)\n",
        "test_set = set(test_ids)\n",
        "\n",
        "def determine_set(row):\n",
        "    if row['Report_ID'] in validation_set:\n",
        "        return 'Validation'\n",
        "    elif row['Report_ID'] in test_set:\n",
        "        return 'Test'\n",
        "    else:\n",
        "        return 'Train'\n",
        "\n",
        "# create a new column indicating the set\n",
        "df['Set'] = df.apply(determine_set, axis=1)\n",
        "\n",
        "# Split the dataframe into training, validation, and test sets\n",
        "train_df = df[df['Set'] == 'Train']\n",
        "validation_df = df[df['Set'] == 'Validation']\n",
        "test_df = df[df['Set'] == 'Test']\n",
        "\n",
        "# Drop the 'Set' column as it's no longer needed\n",
        "train_df.drop(columns=['Set'], inplace=True)\n",
        "validation_df.drop(columns=['Set'], inplace=True)\n",
        "test_df.drop(columns=['Set'], inplace=True)\n",
        "\n",
        "# save the split datasets to new CSV files\n",
        "train_df.to_csv('train_dataset.csv', index=False)\n",
        "validation_df.to_csv('validation_dataset.csv', index=False)\n",
        "test_df.to_csv('test_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD9LZT_Mmvg6",
        "outputId": "83595139-6923-4381-8fd3-aad6dedafcab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a911d33bd56b>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df.drop(columns=['Set'], inplace=True)\n",
            "<ipython-input-1-a911d33bd56b>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  validation_df.drop(columns=['Set'], inplace=True)\n",
            "<ipython-input-1-a911d33bd56b>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df.drop(columns=['Set'], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format CADEC spans to ADR: ---;"
      ],
      "metadata": {
        "id": "maVJgvEdwGRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/CADEC_Validation.csv\")\n",
        "\n",
        "# Because CADEC has some NaN spans, I added condition for that\n",
        "def format_spans(row):\n",
        "    spans = str(row[\"Spans\"])\n",
        "    if spans != \"nan\":  # Check if the value is not NaN\n",
        "        spans = spans.split(\"\\n\")\n",
        "        formatted_spans = \"; \".join([f\"disorder: {span}\" for span in spans])\n",
        "        return formatted_spans\n",
        "    else:\n",
        "        return \"\"  # empty string if the value is NaN\n",
        "\n",
        "df[\"Spans\"] = df.apply(format_spans, axis=1)\n",
        "\n",
        "new_df = df[[\"Text\", \"Spans\"]]\n",
        "new_df.to_csv(\"preprocessed_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "7QZXPI0EwCqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}