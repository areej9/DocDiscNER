{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONB2tI4nwt4r",
        "outputId": "a754542c-728e-4d23-d869-68218d7c22f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## langchain. RECURSIVE ########################"
      ],
      "metadata": {
        "id": "PkwKULvK4y0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "Np2_V4ZK38-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
      ],
      "metadata": {
        "id": "owNAwVqK3zmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk size is measured: by number of characters.\n",
        "\n",
        "text = \"\" #if you want to try one example befor iterating all examples\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2500,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        "    is_separator_regex=True\n",
        ")\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "M_QntRGJ3ziY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All outputs have been cleared to maintain the confidentiality of ShARe-13 and ShARe-14.\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "kHa2HYcxlbFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "5vt03Ahy4lMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate the Recursive splitter through all texts:\n",
        "\n",
        "import pandas as pd\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DocDiscNER/Preprocessed_Datasets/ShaRe14/Train-Doc-ShaRe-14.csv\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\n",
        "        \"\\n\\n\", # removed the \" \" separator, cuz it will allow splitting potential spans!\n",
        "        \"\\n\",\n",
        "        \". \", \"] \"],\n",
        "    is_separator_regex=False\n",
        ")\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Iterate over each row\n",
        "for index, row in df.iterrows():\n",
        "    text = row[\"Text\"]\n",
        "    # Perform text splitting\n",
        "    docs = text_splitter.create_documents([text])\n",
        "\n",
        "    # Iterate over each chunk and its index\n",
        "    for i, doc in enumerate(docs):\n",
        "        rows.append({\"Index\": index, \"Chunk\": doc.page_content})\n",
        "\n",
        "df_output = pd.DataFrame(rows)\n",
        "df_output.to_csv(\"output_chunks.csv\", index=False)\n",
        "# Now, skip the embedding-based chunking code blocks and proceed to (merging the spans with the chunks)."
      ],
      "metadata": {
        "id": "yflPvtamVdfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################## langchain. EMBEDDING-BASED CHUNCKING ########################"
      ],
      "metadata": {
        "id": "P2SW2s1-lvOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Semantic Chunker using OpenAI Embeddings\n",
        "#Code is adapted from: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/semantic-chunker/"
      ],
      "metadata": {
        "id": "3B0u2W6I5DW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain_experimental\n",
        "!pip install --quiet langchain_experimental langchain_openai"
      ],
      "metadata": {
        "id": "0tl4FuiLl3Yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbaa34f-fdb2-4589-a8db-f9ad77c5d750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.4/310.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.6/973.6 kB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "YriVPMLPMlvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"--\""
      ],
      "metadata": {
        "id": "sqA6rNy3q1BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\" #if you want to try one example, put it here"
      ],
      "metadata": {
        "id": "1L129ZPvoREQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SemanticChunker(OpenAIEmbeddings(),breakpoint_threshold_type=\"percentile\",\n",
        "                                breakpoint_threshold_amount=95) # here can add: breakpoint_threshold_type=\"percentile\",\"standard_deviation\",\"interquartile\"\n",
        "docs = text_splitter.create_documents([text])\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "jwgKwHL_5fBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs)) # to know how many chuncks are there"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgx40shH6LxZ",
        "outputId": "a9441ab9-4199-45d5-81b3-dbdc8dda3437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for doc in docs:\n",
        "  print(counter)\n",
        "  print(doc.page_content)\n",
        "  counter+=1"
      ],
      "metadata": {
        "id": "TqzNlPEZ5y8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LangChain Semantic Splitter iterates through all text\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DocDiscNER/Preprocessed_Datasets/ShaRe14/Train-Doc-ShaRe-14.csv\")\n",
        "rows = []\n",
        "\n",
        "# Iterate over each row\n",
        "for index, row in df.iterrows():\n",
        "    text = row[\"Text\"]\n",
        "    # Perform text splitting\n",
        "    docs = text_splitter.create_documents([text])\n",
        "\n",
        "    # Iterate over each chunk and its index\n",
        "    for i, doc in enumerate(docs):\n",
        "        rows.append({\"Index\": index, \"Chunk\": doc.page_content})\n",
        "\n",
        "df_output = pd.DataFrame(rows)\n",
        "df_output.to_csv(\"trainoutput_chunks.csv\", index=False)"
      ],
      "metadata": {
        "id": "0Eg6ogRb5svi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### merging the spans with the chunks.#################"
      ],
      "metadata": {
        "id": "sTv97h0LIzQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Removing null chunks\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/trainoutput_chunks.csv\")\n",
        "\n",
        "df_filtered = df[df['Chunk'].notnull()]\n",
        "df_filtered.to_csv(\"/content/trainoutput_chunks.csv\", index=False)"
      ],
      "metadata": {
        "id": "1ZKqWC39t9MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we merge the spans with the chunks\n",
        "import pandas as pd\n",
        "\n",
        "# take the result of chunking output\n",
        "file1 = pd.read_csv(\"/content/trainoutput_chunks.csv\")\n",
        "# this file2 is the resulting file of DocDiscNER_Preprocessing.ipynb. We use it here as the reference for spans.\n",
        "file2 = pd.read_csv(\"/content/drive/MyDrive/DocDiscNER/Preprocessed_Datasets/ShaRe14/Train-Doc-ShaRe-14.csv\")\n",
        "\n",
        "# Function to calculate offsets for each chunk\n",
        "def calculate_offsets(chunks):\n",
        "    offsets = []\n",
        "    start_offset = 0\n",
        "    for chunk in chunks:\n",
        "        end_offset = start_offset + len(chunk) - 1\n",
        "        offsets.append(f\"{start_offset}-{end_offset}\")\n",
        "        start_offset = end_offset + 2  # Add 2 for the newline characters\n",
        "    return offsets\n",
        "\n",
        "file1['ChunkOffsets'] = file1.groupby('Index')['Chunk'].transform(calculate_offsets)\n",
        "\n",
        "matched_chunks = {}\n",
        "\n",
        "for index, row in file2.iterrows():\n",
        "    # Get the index value for the document\n",
        "    doc_index = index\n",
        "\n",
        "    spans = row['Spans'].split('\\n')  # Split the spans by newline\n",
        "    offsets = row['Offsets'].split('\\n')  # Convert offsets from string to list\n",
        "\n",
        "    # Filter chunks from file1 that have the same index as the current document\n",
        "    chunks = file1[file1['Index'] == doc_index]['Chunk'].tolist()\n",
        "    chunk_offsets = file1[file1['Index'] == doc_index]['ChunkOffsets'].tolist()\n",
        "\n",
        "    # Iterate through file1 to find the correct chunk\n",
        "    for chunk, chunk_offset in zip(chunks, chunk_offsets):\n",
        "        chunk_spans = []\n",
        "        chunk_start, chunk_end = map(int, chunk_offset.split('-'))  # Get chunk start and end\n",
        "        for span, offset in zip(spans, offsets):\n",
        "            start = offset.split('-', 1)[0] # Extract start offsets\n",
        "            start = int(start)\n",
        "            # Check if the span falls within this chunk\n",
        "            if start>= chunk_start and start <= chunk_end:\n",
        "                chunk_spans.append(span)\n",
        "                #break  # Move to the next span\n",
        "\n",
        "        if chunk_spans:\n",
        "           matched_chunks[chunk] = chunk_spans\n",
        "\n",
        "# Create a DataFrame from the matched_chunks dictionary\n",
        "#df_matched_chunks = pd.DataFrame({'Text': list(matched_chunks.keys()), 'Spans': list(matched_chunks.values())})\n",
        "df_matched_chunks = pd.DataFrame({'Text': list(matched_chunks.keys()), 'Spans': ['; '.join(value) for value in matched_chunks.values()]})\n",
        "\n",
        "df_matched_chunks.to_csv(\"matched_chunks.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Gq0gl06Hjl7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we preprocess the resulting chunks into the DocDiscNER text and spans representation.\n",
        "# After chunking ShARe-13, we can optionally clean the text by removing excess details like date/time.\n",
        "# Additionally, format spans with the \"disorder:---\" prefix.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/matched_chunks.csv\")\n",
        "\n",
        "#  remove time and date expressions and symbols\n",
        "def remove_time_date_symbols(text):\n",
        "    # Remove time and date expressions\n",
        "    text = re.sub(r\"\\[?\\*\\*\\d{4}-\\d{2}-\\d{2}\\*\\*\\]?\\s?(at)?\\s?\\d{2}:\\d{2}(AM|PM)?\", \"\", text)\n",
        "    pattern = re.compile(r'\\[\\*\\*.*?\\*\\*\\]')\n",
        "    # Replace all occurrences of the pattern with an empty string\n",
        "    text = re.sub(pattern, '', text)\n",
        "    # Remove symbols\n",
        "    text = text.replace(\"___________________\", \"\").replace(\"||||     ||||\", \"\")\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the \"Texts\" column\n",
        "df[\"Text\"] = df[\"Text\"].apply(remove_time_date_symbols)\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "\n",
        "# format spans\n",
        "def format_spans(row):\n",
        "    spans = row[\"Spans\"].split(\";\")\n",
        "    formatted_spans = \"; \".join([f\"disorder: {span.strip()}\" for span in spans])\n",
        "    return formatted_spans\n",
        "\n",
        "# Apply formatting to the \"Spans\" column\n",
        "df[\"Spans\"] = df.apply(format_spans, axis=1)\n",
        "\n",
        "# Create \"Text\" and \"Spans\" columns\n",
        "new_df = df[[\"Text\", \"Spans\"]]\n",
        "\n",
        "new_df.to_csv(\"Train-Doc-ShaRe-14.csv\", index=False)"
      ],
      "metadata": {
        "id": "OSii8lvZNCJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionall, if you want to print the number of tokens in each text for verification."
      ],
      "metadata": {
        "id": "DrD1bcp3pLrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to count tokens in a text\n",
        "def count_tokens(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return len(tokens)\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/Train-Doc-ShaRe-14.csv')\n",
        "\n",
        "# Calculate the number of tokens in the 'Text' column and create a new 'TextLength' column\n",
        "df['TextLength'] = df['Text'].apply(lambda x: count_tokens(str(x)))\n",
        "\n",
        "# Save the DataFrame with the new column to a new CSV file\n",
        "df.to_csv('Train14_length.csv', index=False)\n",
        "\n",
        "print(\"Number of tokens in each text have been calculated and saved to 'your_dataset_with_length.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nkUVPawWYy9",
        "outputId": "d6f66819-fa55-46b7-dd91-bf12ddc5fd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in each text have been calculated and saved to 'your_dataset_with_length.csv'\n"
          ]
        }
      ]
    }
  ]
}