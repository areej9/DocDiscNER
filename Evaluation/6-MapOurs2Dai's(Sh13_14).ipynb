{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This code converts our document-level results to an inline format compatible with Dai et al., 2020.\n",
        "# It also includes cloning Dai's evaluation script for calculating results."
      ],
      "metadata": {
        "id": "GCZmH7qquTaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGtjNcQaeM0J",
        "outputId": "3af2148c-d7c1-4416-80f2-c169d9af5b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import csv\n",
        "import re\n",
        "\n",
        "def is_header(text):\n",
        "    # ShaRe14:\n",
        "    #pattern = r'.* DISCHARGE_SUMMARY .*'\n",
        "    # ShaRe13:\n",
        "    pattern = r'.*\\s(DISCHARGE_SUMMARY|RADIOLOGY_REPORT|ECHO_REPORT)\\s.*'\n",
        "    return bool(re.match(pattern, text.strip()))\n",
        "\n",
        "def assemble_documents(csv_file_path):\n",
        "    documents = dict()\n",
        "    current_doc = []\n",
        "    current_labels = []\n",
        "    current_doc_id = None\n",
        "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip the header row\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if len(row) < 1:\n",
        "                continue\n",
        "\n",
        "            text = row[0]\n",
        "            labels = row[2]\n",
        "            if is_header(text):\n",
        "                if current_doc:\n",
        "                    documents[current_doc_id]= ('\\n'.join(current_doc), current_labels)\n",
        "                current_doc = [text]\n",
        "                current_labels = [labels]\n",
        "                current_doc_id = text.split('|')[0].strip()\n",
        "            else:\n",
        "                current_doc.append(text)\n",
        "                current_labels.append(labels)\n",
        "\n",
        "        # Add the last document if it exists\n",
        "        if current_doc:\n",
        "            documents[current_doc_id] = ('\\n'.join(current_doc), current_labels)\n",
        "\n",
        "    return documents\n",
        "# provide path of DocDiscNER_NER or DocDiscNER_SC\n",
        "csv_file_path = '/content/ShaRe13-MajVoteOf3.csv'\n",
        "assembled_documents = assemble_documents(csv_file_path)\n",
        "print(len(assembled_documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nArDI-qeM0M"
      },
      "outputs": [],
      "source": [
        "def process_input_file(input_file_path):\n",
        "    documents = {}\n",
        "    current_doc_id = None\n",
        "    current_doc_sentences = []\n",
        "    current_doc_annotations = []\n",
        "\n",
        "    with open(input_file_path, 'r') as f:\n",
        "        while True:\n",
        "            try:\n",
        "                sentence = next(f).strip()\n",
        "                gold = next(f).strip()\n",
        "                gold = gold.split(\"|\") if len(gold) > 0 else []\n",
        "                assert len(next(f).strip()) == 0  # Empty line\n",
        "\n",
        "                if is_header(sentence):\n",
        "                    if current_doc_id:\n",
        "                        documents[current_doc_id] = {\n",
        "                            'sentences': current_doc_sentences,\n",
        "                            'annotations': current_doc_annotations,\n",
        "                            'predictions': assembled_documents[current_doc_id]\n",
        "                        }\n",
        "                    current_doc_id = sentence.split('||||')[0].strip()\n",
        "                    current_doc_sentences = [sentence]\n",
        "                    current_doc_annotations = [gold]\n",
        "                else:\n",
        "                    current_doc_sentences.append(sentence)\n",
        "                    current_doc_annotations.append(gold)\n",
        "\n",
        "            except StopIteration:\n",
        "                # End of file reached\n",
        "                if current_doc_id:\n",
        "                    documents[current_doc_id] = {\n",
        "                        'sentences': current_doc_sentences,\n",
        "                        'annotations': current_doc_annotations,\n",
        "                        'predictions': assembled_documents[current_doc_id]\n",
        "                    }\n",
        "                break\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myhRBbTDeM0N"
      },
      "outputs": [],
      "source": [
        "def split_disorders(disorder_list, case_sensitive=False):\n",
        "    all_disorders = []\n",
        "    for disorder_string in disorder_list:\n",
        "        # Split by semicolon and strip whitespace\n",
        "        disorders = [d.strip() for d in disorder_string.split(';')]\n",
        "        # Extract the text after \"disorder:\" for each item\n",
        "        disorders = [d.split('disorder:')[-1].strip().replace('â€™', \"'\").replace(\"'\", \" '\").replace('-', ' - ').replace(',', ' ,').replace('.', ' . ').replace('/', ' / ').replace('&', ' & ') for d in disorders]\n",
        "        if not case_sensitive:\n",
        "            disorders = [d.lower() for d in disorders]\n",
        "        all_disorders.extend(disorders)\n",
        "    return all_disorders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GzNY7fFeM0N"
      },
      "outputs": [],
      "source": [
        "def find_disorder_spans_disc(input_tokens, disorders, case_sensitive=False):\n",
        "    spans = []\n",
        "    mentions = []\n",
        "\n",
        "    def normalize(text):\n",
        "        return text if case_sensitive else text.lower().replace(\"*\", \"\")\n",
        "\n",
        "    # Tokenize disorders\n",
        "    tokenized_disorders = {normalize(d): d.split() for d in disorders}\n",
        "\n",
        "    for i in range(len(input_tokens)):\n",
        "        for j in range(i, len(input_tokens)):\n",
        "            span = ' '.join(input_tokens[i:j+1])\n",
        "            normalized_span = normalize(span)\n",
        "\n",
        "            # Check for full span match\n",
        "            if normalized_span in tokenized_disorders:\n",
        "                spans.append(f\"{i},{j} Disorder\")\n",
        "                mentions.append(span)\n",
        "\n",
        "    return spans, mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liAjtxAXeM0O",
        "outputId": "55d5e206-fd92-4405-c6ac-9d9ddf7e0a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0), (6, 7), (9, 10)]\n"
          ]
        }
      ],
      "source": [
        "def merge_consecutive_spans(spans):\n",
        "    if not spans:\n",
        "        return []\n",
        "\n",
        "    # Sort the spans based on the start index\n",
        "    sorted_spans = sorted(spans, key=lambda x: x[0])\n",
        "\n",
        "    merged = [sorted_spans[0]]\n",
        "\n",
        "    for current in sorted_spans[1:]:\n",
        "        last = merged[-1]\n",
        "\n",
        "        # If the current span starts immediately after the last one ends\n",
        "        if current[0] == last[1] + 1:\n",
        "            # Merge by updating the end of the last span\n",
        "            merged[-1] = (last[0], current[1])\n",
        "        else:\n",
        "            # If not consecutive, add as a new span\n",
        "            merged.append(current)\n",
        "\n",
        "    return merged\n",
        "\n",
        "# Example usage\n",
        "spans = [(0,0), (6,6), (7,7), (9,9), (10,10)]\n",
        "result = merge_consecutive_spans(spans)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DpKV7mpeM0O",
        "outputId": "85802d8b-7278-4e90-94ff-b54c580e05f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Abd ND': [(0, 0), (6, 6)], 'Abd NT': [(0, 0), (4, 4)], 'Abd S': [(0, 0), (2, 2)], 'Abd surgical scars': [(0, 0), (15, 16)]}\n"
          ]
        }
      ],
      "source": [
        "def find_discontinuous_mentions(sentence, mentions, case_sensitive=True):\n",
        "    tokens = sentence.split()\n",
        "    results = {}\n",
        "\n",
        "    if not case_sensitive:\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    for mention in mentions:\n",
        "        mention_parts = mention.split()\n",
        "        if not case_sensitive:\n",
        "            mention_parts = [part.lower() for part in mention_parts]\n",
        "\n",
        "        spans = []\n",
        "        found_all = True\n",
        "\n",
        "        for part in mention_parts:\n",
        "            try:\n",
        "                index = tokens.index(part)\n",
        "                spans.append((index, index))\n",
        "            except ValueError:\n",
        "                found_all = False\n",
        "                break\n",
        "\n",
        "        if found_all:\n",
        "            results[mention] = merge_consecutive_spans(spans)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence = \"Abd : S / NT / ND , + BS , - HSM , multiple surgical scars .\"\n",
        "mentions = ['Abd ND', 'Abd NT', 'Abd S', 'Abd surgical scars']\n",
        "\n",
        "result = find_discontinuous_mentions(sentence, mentions)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SPiD8q_eM0P",
        "outputId": "da62b1d9-8836-4c89-8875-668922ced70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# the path here is for the gold test set preprocessed in the Dai et al., 2020 format.\n",
        "test_file = '/content/test.txt'\n",
        "processed_documents = process_input_file(test_file)\n",
        "\n",
        "missing = 0\n",
        "\n",
        "out_file = \"\"\n",
        "\n",
        "# Print the processed documents\n",
        "for doc_id, doc_data in processed_documents.items():\n",
        "    # print(f\"Document ID: {doc_id}\")\n",
        "    # print(\"Sentences:\")\n",
        "    all_preds = split_disorders(doc_data['predictions'][1], case_sensitive=True)\n",
        "    missing_preds = all_preds\n",
        "    output = []\n",
        "    # print(all_preds)\n",
        "    for annotation, sentence in zip(doc_data['annotations'], doc_data['sentences']):\n",
        "        spans, matches = find_disorder_spans_disc(sentence.split(), all_preds, case_sensitive=True)\n",
        "        # print(f\"{sentence}\")\n",
        "        # print(f\"g: {annotation}\")\n",
        "        # print(f\"p: {spans}\")\n",
        "        output.append((sentence, spans, annotation))\n",
        "        missing_preds = [p for p in missing_preds if p not in matches]\n",
        "\n",
        "    # print(f\"Missing predictions: {missing_preds}\")\n",
        "    if missing_preds:\n",
        "        for i, sent in enumerate(doc_data['sentences']):\n",
        "            mentions = find_discontinuous_mentions(sent, missing_preds, case_sensitive=True)\n",
        "            if mentions:\n",
        "                for mention in mentions.values():\n",
        "                    output[i][1].append(','.join(f\"{s},{e}\" for s,e in mention) +\" Disorder\")\n",
        "                # print(output[i])\n",
        "                missing_preds = [p for p in missing_preds if p not in mentions.keys()]\n",
        "                if not missing_preds:\n",
        "                    break\n",
        "    if missing_preds:\n",
        "        for i, sent in enumerate(doc_data['sentences']):\n",
        "            spans, matches = find_disorder_spans_disc(sent.split(), missing_preds, case_sensitive=False)\n",
        "            if spans:\n",
        "                for span in spans:\n",
        "                    output[i][1].extend(spans)\n",
        "                # print(output[i])\n",
        "                missing_preds = [p for p in missing_preds if p.lower() not in set(m.lower().replace(\"*\", \"\")  for m in matches)]\n",
        "                if not missing_preds:\n",
        "                    break\n",
        "    if missing_preds:\n",
        "        for i, sent in enumerate(doc_data['sentences']):\n",
        "            mentions = find_discontinuous_mentions(sent, missing_preds, case_sensitive=False)\n",
        "            if mentions:\n",
        "                for mention in mentions.values():\n",
        "                    output[i][1].append(','.join(f\"{s},{e}\" for s,e in mention) +\" Disorder\")\n",
        "                # print(output[i])\n",
        "                missing_preds = [p for p in missing_preds if p.lower() not in set(m.lower() for m in mentions.keys())]\n",
        "                if not missing_preds:\n",
        "                    break\n",
        "    for sent, spans, gt in output:\n",
        "        out_file += sent + '\\n'\n",
        "        out_file += '|'.join(spans) +  '\\n\\n'\n",
        "    # print(out_file)\n",
        "    # break\n",
        "\n",
        "    # if missing_preds:\n",
        "    #     print(doc_id)\n",
        "    #     print(doc_data['sentences'])\n",
        "    #     print(f\"Missing predictions: {missing_preds}\")\n",
        "    #     missing += len(missing_preds)\n",
        "        # assert False\n",
        "        # break\n",
        "        # print('\\n'.join(doc_data['sentences']))\n",
        "print(missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3G2ngL5eM0P"
      },
      "outputs": [],
      "source": [
        "with open('ours_to_dai.txt', 'w+') as f:\n",
        "    f.write(out_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/dainlp/acl2020-transition-discontinuous-ner.git\n",
        "\n",
        "# Navigate into the directory\n",
        "%cd xdai/ner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S_jNFGGqQTS",
        "outputId": "dfeaf65b-7b1f-42c3-a1b2-4e037a98c309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'acl2020-transition-discontinuous-ner'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (109/109), 423.43 KiB | 3.31 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "[Errno 2] No such file or directory: 'xdai/ner'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/acl2020-transition-discontinuous-ner/code/xdai/ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl8hNIcUqp2X",
        "outputId": "45ae55e5-a253-4f0e-ccb8-bb8e4fa60ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/acl2020-transition-discontinuous-ner/code/xdai/ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rename (ours_to_dai.txt) to (test.txt) and put it in folder /pred/test.txt\n",
        "# put the gold which is in Dai's format in folder /gold/test.txt\n",
        "!python evaluate.py --gold_filepath /content/gold/test.txt --pred_filepath /content/pred/test.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNrwHMn7n9ew",
        "outputId": "1bfa6cea-8d12-4b8d-9eb8-74d324f93064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "micro-precision 0.8140188057149835\n",
            "micro-recall 0.8414541782378188\n",
            "micro-f1 0.8275091552355534\n",
            "sentences_with_disc-micro-precision 0.6760037348272643\n",
            "sentences_with_disc-micro-recall 0.7218344965104686\n",
            "sentences_with_disc-micro-f1 0.6981677917068466\n",
            "disc-mention-micro-precision 0.5098591549295775\n",
            "disc-mention-micro-recall 0.6407079646017699\n",
            "disc-mention-micro-f1 0.567843137254902\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "transformers-mlc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}